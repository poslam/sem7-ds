{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e77dc22",
   "metadata": {},
   "source": [
    "## 1 - 19.09.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import joblib\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758496a",
   "metadata": {},
   "source": [
    "load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"../../../data/ml_01_1.csv\")\n",
    "\n",
    "test = pd.read_csv(\"../../../data/ml_01_2.csv\")\n",
    "train = pd.read_csv(\"../../../data/ml_01_3.csv\")\n",
    "\n",
    "train.drop(columns=[\"PassengerId\"], inplace=True)\n",
    "test.drop(columns=[\"PassengerId\"], inplace=True)\n",
    "\n",
    "\n",
    "def fe(df: pd.DataFrame):\n",
    "    df[\"TotalServiceSpend\"] = (\n",
    "        df[\"RoomService\"]\n",
    "        + df[\"FoodCourt\"]\n",
    "        + df[\"ShoppingMall\"]\n",
    "        + df[\"Spa\"]\n",
    "        + df[\"VRDeck\"]\n",
    "    )\n",
    "    df[\"ServiceSpendPerAge\"] = df[\"TotalServiceSpend\"] / (df[\"Age\"] + 1)\n",
    "    df[[\"CabinDeck\", \"CabinNum\", \"CabinSide\"]] = df[\"Cabin\"].str.split(\n",
    "        \"/\", expand=True\n",
    "    )\n",
    "    df[\"LastName\"] = df[\"Name\"].str.split().str[-1]\n",
    "    df[\"IsFamily\"] = df.groupby(\"LastName\")[\"Name\"].transform(\"size\") > 1\n",
    "    df[\"AgeGroup\"] = pd.cut(\n",
    "        df[\"Age\"],\n",
    "        bins=[0, 18, 30, 50, 80],\n",
    "        labels=[\"Child\", \"YoungAdult\", \"Adult\", \"Senior\"],\n",
    "    )\n",
    "    df[\"TravelAlone\"] = (\n",
    "        df[\"Cabin\"].duplicated(keep=False) | df[\"IsFamily\"]\n",
    "    ).astype(int)\n",
    "    df[\"CryoSleepAndSpent\"] = (\n",
    "        df[\"CryoSleep\"] & (df[\"TotalServiceSpend\"] > 0)\n",
    "    ).astype(int)\n",
    "    df[\"VIPSpendMultiplier\"] = df[\"VIP\"] * df[\"TotalServiceSpend\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = fe(train)\n",
    "test = fe(test)\n",
    "\n",
    "\n",
    "def impute(df: pd.DataFrame):\n",
    "    categorical_cols = [\n",
    "        \"HomePlanet\",\n",
    "        \"CryoSleep\",\n",
    "        \"Cabin\",\n",
    "        \"Destination\",\n",
    "        \"AgeGroup\",\n",
    "        \"CabinDeck\",\n",
    "    ]\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].dtype.name == \"category\":\n",
    "            if \"NoInformation\" not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories(\"NoInformation\")\n",
    "            df[col].fillna(\"NoInformation\", inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(\"NoInformation\", inplace=True)\n",
    "\n",
    "    vip_threshold = df[\"VIPSpendMultiplier\"].quantile(0.99)\n",
    "    df[\"VIP\"].fillna(df[\"VIPSpendMultiplier\"] > vip_threshold, inplace=True)\n",
    "    df[\"Name\"].fillna(\"noName\", inplace=True)\n",
    "    df[\"LastName\"].fillna(\"noName\", inplace=True)\n",
    "    df[\"CabinSide\"].fillna(df[\"CabinSide\"].mode()[0], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = impute(train)\n",
    "test = impute(test)\n",
    "\n",
    "num_c = [\n",
    "    \"Age\",\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "    \"TotalServiceSpend\",\n",
    "    \"ServiceSpendPerAge\",\n",
    "    \"CabinNum\",\n",
    "    \"VIPSpendMultiplier\",\n",
    "]\n",
    "\n",
    "I = IterativeImputer(\n",
    "    random_state=0,\n",
    "    n_nearest_features=None,\n",
    "    initial_strategy=\"mean\",\n",
    ")\n",
    "train[num_c] = I.fit_transform(train[num_c])\n",
    "test[num_c] = I.transform(test[num_c])\n",
    "\n",
    "\n",
    "def update(df: pd.DataFrame):\n",
    "    cat_c = [\n",
    "        \"HomePlanet\",\n",
    "        \"CryoSleep\",\n",
    "        \"Cabin\",\n",
    "        \"Destination\",\n",
    "        \"Name\",\n",
    "        \"CabinDeck\",\n",
    "        \"CabinSide\",\n",
    "        \"LastName\",\n",
    "        \"VIP\",\n",
    "        \"IsFamily\",\n",
    "        \"AgeGroup\",\n",
    "    ]\n",
    "\n",
    "    for col in cat_c:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "\n",
    "train.drop([\"Name\", \"Cabin\", \"LastName\"], axis=1, inplace=True)\n",
    "test.drop([\"Name\", \"Cabin\", \"LastName\"], axis=1, inplace=True)\n",
    "\n",
    "cols_encode = [\n",
    "    \"HomePlanet\",\n",
    "    \"CryoSleep\",\n",
    "    \"Destination\",\n",
    "    \"CabinDeck\",\n",
    "    \"CabinSide\",\n",
    "    \"VIP\",\n",
    "    \"IsFamily\",\n",
    "    \"AgeGroup\",\n",
    "]\n",
    "\n",
    "train = pd.get_dummies(train, columns=cols_encode).astype(int)\n",
    "test = pd.get_dummies(test, columns=cols_encode).astype(int)\n",
    "\n",
    "all_cols = train.columns\n",
    "cols_Scale = all_cols.drop(\"Transported\")\n",
    "\n",
    "Sc = StandardScaler()\n",
    "train[cols_Scale] = Sc.fit_transform(train[cols_Scale])\n",
    "test[cols_Scale] = Sc.transform(test[cols_Scale])\n",
    "\n",
    "final_cols = [\n",
    "    \"Age\",\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "    \"TotalServiceSpend\",\n",
    "    \"ServiceSpendPerAge\",\n",
    "    \"CabinNum\",\n",
    "    \"TravelAlone\",\n",
    "    \"CryoSleepAndSpent\",\n",
    "    \"VIPSpendMultiplier\",\n",
    "    \"HomePlanet_Earth\",\n",
    "    \"HomePlanet_Europa\",\n",
    "    \"HomePlanet_Mars\",\n",
    "    \"HomePlanet_NoInformation\",\n",
    "    \"CryoSleep_False\",\n",
    "    \"CryoSleep_True\",\n",
    "    \"CryoSleep_NoInformation\",\n",
    "    \"Destination_55 Cancri e\",\n",
    "    \"Destination_NoInformation\",\n",
    "    \"Destination_PSO J318.5-22\",\n",
    "    \"Destination_TRAPPIST-1e\",\n",
    "    \"CabinDeck_A\",\n",
    "    \"CabinDeck_B\",\n",
    "    \"CabinDeck_C\",\n",
    "    \"CabinDeck_D\",\n",
    "    \"CabinDeck_E\",\n",
    "    \"CabinDeck_F\",\n",
    "    \"CabinDeck_G\",\n",
    "    \"CabinDeck_NoInformation\",\n",
    "    \"CabinDeck_T\",\n",
    "    \"CabinSide_P\",\n",
    "    \"CabinSide_S\",\n",
    "    \"VIP_False\",\n",
    "    \"VIP_True\",\n",
    "    \"IsFamily_False\",\n",
    "    \"IsFamily_True\",\n",
    "    \"AgeGroup_Child\",\n",
    "    \"AgeGroup_YoungAdult\",\n",
    "    \"AgeGroup_Adult\",\n",
    "    \"AgeGroup_Senior\",\n",
    "    \"AgeGroup_NoInformation\",\n",
    "    \"Transported\",\n",
    "]\n",
    "\n",
    "train = train[final_cols]\n",
    "\n",
    "train_N = train.copy()\n",
    "train_N.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccee96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55934e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be79c67",
   "metadata": {},
   "source": [
    "modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e794a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"Transported\", axis=1)\n",
    "y = train[\"Transported\"]\n",
    "\n",
    "xn = train_N.drop(\"Transported\", axis=1)\n",
    "yn = train_N[\"Transported\"]\n",
    "\n",
    "\n",
    "def Train_ML(X=None, y=None, test=None, model=None, xn=None, yn=None):\n",
    "\n",
    "    if X is None or y is None:\n",
    "        if xn is not None and yn is not None:\n",
    "            X = xn\n",
    "            y = yn\n",
    "        else:\n",
    "            raise ValueError(\"Either (X, y) or (xn, yn) must be provided\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    test_probs = np.zeros(len(test))\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        train_pred = (train_proba >= 0.5).astype(int)\n",
    "        test_pred = (test_proba >= 0.5).astype(int)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        test_probs += model.predict_proba(test)[:, 1] / n_splits\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold}: Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"__Mean Train Accuracy: {np.mean(train_accuracies):.4f}\")\n",
    "    print(f\"__Mean Test Accuracy: {np.mean(test_accuracies):.4f}\")\n",
    "\n",
    "    return test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lb : 0.80196\"\"\"\n",
    "\n",
    "Lp = {\n",
    "    \"learning_rate\": 0.05255662080483515,\n",
    "    \"max_depth\": 34,\n",
    "    \"reg_alpha\": 2.226949211863629,\n",
    "    \"reg_lambda\": 0.10506605571368621,\n",
    "    \"num_leaves\": 39,\n",
    "    \"subsample\": 0.22249538144765668,\n",
    "    \"colsample_bytree\": 0.8736761308217752,\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_iter\": 200,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "\n",
    "\n",
    "lm = LGBMClassifier(**Lp, verbose=-1, random_state=SEED)\n",
    "\n",
    "print(\"---> LGB__0\")\n",
    "lpreds = Train_ML(X, y, test, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LB : 0.80149\"\"\"\n",
    "\n",
    "xp = {\n",
    "    \"learning_rate\": 0.06516652353739706,\n",
    "    \"max_depth\": 38,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"gamma\": 0.008447929795037001,\n",
    "    \"subsample\": 0.8844407112693309,\n",
    "    \"colsample_bytree\": 0.36615109453559186,\n",
    "    \"lambda\": 1.3602165377656108,\n",
    "    \"alpha\": 8.743278468315916,\n",
    "}\n",
    "\n",
    "xm = XGBClassifier(\n",
    "    **xp,\n",
    "    verbose=0,\n",
    "    random_state=SEED,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_estimators=200,\n",
    ")\n",
    "x_preds = Train_ML(X, y, test, xm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe210712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LB : 0.80266\"\"\"\n",
    "\n",
    "cp = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.03467456378123331,\n",
    "    \"depth\": 6,\n",
    "    \"random_strength\": 2.625413605618935,\n",
    "    \"min_data_in_leaf\": 88,\n",
    "}\n",
    "\n",
    "\n",
    "cm = CatBoostClassifier(**cp, verbose=0, random_state=SEED)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"---> Cat__0\")\n",
    "c_preds = Train_ML(X, y, test, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a3356",
   "metadata": {},
   "source": [
    "## 2 - 06.10.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "# Download a sample image if it doesn't exist\n",
    "image_path = (\n",
    "    \"/Users/poslam/Downloads/projects/fefu/7/ds/ml/labs/2/sample_image.jpg\"\n",
    ")\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324500f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, mask = cv2.threshold(gray, 170, 200, cv2.THRESH_BINARY)\n",
    "\n",
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mask based on brightness threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b32bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipeline\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.GaussNoise(),\n",
    "                A.GaussNoise(),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.MotionBlur(p=0.2),\n",
    "                A.MedianBlur(blur_limit=3, p=0.1),\n",
    "                A.Blur(blur_limit=3, p=0.1),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.OpticalDistortion(p=0.3),\n",
    "                A.GridDistortion(p=0.1),\n",
    "                A.PiecewiseAffine(p=0.3),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(clip_limit=2),\n",
    "                A.Sharpen(),\n",
    "                A.Emboss(),\n",
    "                A.RandomBrightnessContrast(),\n",
    "            ],\n",
    "            p=0.3,\n",
    "        ),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create directory for augmented images if it doesn't exist\n",
    "os.makedirs(\"augmented_images\", exist_ok=True)\n",
    "os.makedirs(\"augmented_masks\", exist_ok=True)\n",
    "\n",
    "# Perform augmentations\n",
    "n_augmentations = 5\n",
    "for i in range(n_augmentations):\n",
    "    # Apply augmentation\n",
    "    augmented = transform(image=image, mask=mask)\n",
    "    aug_image = augmented[\"image\"]\n",
    "    aug_mask = augmented[\"mask\"]\n",
    "\n",
    "    # Save augmented images and masks\n",
    "    cv2.imwrite(\n",
    "        f\"augmented_images/aug_{i}.jpg\",\n",
    "        cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR),\n",
    "    )\n",
    "    cv2.imwrite(f\"augmented_masks/mask_{i}.png\", aug_mask)\n",
    "\n",
    "    # Display every 10th augmentation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.imshow(aug_image)\n",
    "    ax1.set_title(f\"Augmented Image {i}\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(aug_mask, cmap=\"gray\")\n",
    "    ax2.set_title(f\"Augmented Mask {i}\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(f\"Augmentation complete!\")\n",
    "print(f\"Generated {n_augmentations} augmented images and masks\")\n",
    "print(f\"Images saved in: {os.path.abspath('augmented_images')}\")\n",
    "print(f\"Masks saved in: {os.path.abspath('augmented_masks')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d83841",
   "metadata": {},
   "source": [
    "## 3 - 06.10.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pymorphy3\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"üòä –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π, –∫–∞–∫ –æ—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤!!!\"\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "text = re.sub(r\"\\d+\", \"\", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "tokens = text.split()\n",
    "\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "clean_text = \" \".join(tokens)\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "print(Counter([morph.parse(i)[0].normal_form for i in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f490015",
   "metadata": {},
   "source": [
    "## 4 - 24.10.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7d355915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as ppc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "badf783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   id                    300000 non-null  int64  \n",
      " 1   Brand                 290295 non-null  object \n",
      " 2   Material              291653 non-null  object \n",
      " 3   Size                  293405 non-null  object \n",
      " 4   Compartments          300000 non-null  float64\n",
      " 5   Laptop Compartment    292556 non-null  object \n",
      " 6   Waterproof            292950 non-null  object \n",
      " 7   Style                 292030 non-null  object \n",
      " 8   Color                 290050 non-null  object \n",
      " 9   Weight Capacity (kg)  299862 non-null  float64\n",
      " 10  Price                 300000 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 25.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/backpack.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "814b7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c9404a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Price\"]\n",
    "X = df.drop(columns=[\"Price\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4123ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Material</th>\n",
       "      <th>Size</th>\n",
       "      <th>Compartments</th>\n",
       "      <th>Laptop Compartment</th>\n",
       "      <th>Waterproof</th>\n",
       "      <th>Style</th>\n",
       "      <th>Color</th>\n",
       "      <th>Weight Capacity (kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jansport</td>\n",
       "      <td>Leather</td>\n",
       "      <td>Medium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Tote</td>\n",
       "      <td>Black</td>\n",
       "      <td>11.611723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jansport</td>\n",
       "      <td>Canvas</td>\n",
       "      <td>Small</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>27.078537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Under Armour</td>\n",
       "      <td>Leather</td>\n",
       "      <td>Small</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Red</td>\n",
       "      <td>16.643760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Nylon</td>\n",
       "      <td>Small</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>12.937220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>Canvas</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>17.749338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand Material    Size  Compartments Laptop Compartment Waterproof  \\\n",
       "0      Jansport  Leather  Medium           7.0                Yes         No   \n",
       "1      Jansport   Canvas   Small          10.0                Yes        Yes   \n",
       "2  Under Armour  Leather   Small           2.0                Yes         No   \n",
       "3          Nike    Nylon   Small           8.0                Yes         No   \n",
       "4        Adidas   Canvas  Medium           1.0                Yes        Yes   \n",
       "\n",
       "       Style  Color  Weight Capacity (kg)  \n",
       "0       Tote  Black             11.611723  \n",
       "1  Messenger  Green             27.078537  \n",
       "2  Messenger    Red             16.643760  \n",
       "3  Messenger  Green             12.937220  \n",
       "4  Messenger  Green             17.749338  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "683ceb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    112.15875\n",
       "1     68.88056\n",
       "2     39.17320\n",
       "3     80.60793\n",
       "4     86.02312\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "85bb50fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∏—Å–ø–µ—Ä—Å–∏—è: 1529.3103490852563\n",
      "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: 39.10639780247289\n"
     ]
    }
   ],
   "source": [
    "variance = np.var(Y)\n",
    "std_dev = np.std(Y)\n",
    "\n",
    "print(\"–î–∏—Å–ø–µ—Ä—Å–∏—è:\", variance)\n",
    "print(\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ:\", std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4366ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"Compartments\",\n",
    "    \"Weight Capacity (kg)\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Brand\",\n",
    "    \"Material\",\n",
    "    \"Size\",\n",
    "    \"Laptop Compartment\",\n",
    "    \"Waterproof\",\n",
    "    \"Style\",\n",
    "    \"Color\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b79151d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", ppc.StandardScaler()),\n",
    "        (\n",
    "            \"transformer\",\n",
    "            ppc.SplineTransformer(\n",
    "                n_knots=3,\n",
    "                degree=3,\n",
    "                order=\"F\",\n",
    "                extrapolation=\"linear\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"encoder\",\n",
    "            ppc.OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                sparse_output=False,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c790b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [100, 200], \"max_depth\": [5, None]}\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"poly\", ppc.PolynomialFeatures(2)),\n",
    "        (\n",
    "            \"regressor\",\n",
    "            GridSearchCV(\n",
    "                RandomForestRegressor(),\n",
    "                cv=2,\n",
    "                n_jobs=5,\n",
    "                param_grid=param_grid,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "655f190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "11415573",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.3,\n",
    "    random_state=69,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "77860509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n",
      "\n",
      "rmse:\t 38.568855910299945\n"
     ]
    }
   ],
   "source": [
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"test: \\n\")\n",
    "print(\"rmse:\\t\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ea250",
   "metadata": {},
   "source": [
    "## 5 - 24.10.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "5dc0b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "e69addb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19219 entries, 0 to 19218\n",
      "Data columns (total 35 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     19219 non-null  int64  \n",
      " 1   X_Minimum              19219 non-null  int64  \n",
      " 2   X_Maximum              19219 non-null  int64  \n",
      " 3   Y_Minimum              19219 non-null  int64  \n",
      " 4   Y_Maximum              19219 non-null  int64  \n",
      " 5   Pixels_Areas           19219 non-null  int64  \n",
      " 6   X_Perimeter            19219 non-null  int64  \n",
      " 7   Y_Perimeter            19219 non-null  int64  \n",
      " 8   Sum_of_Luminosity      19219 non-null  int64  \n",
      " 9   Minimum_of_Luminosity  19219 non-null  int64  \n",
      " 10  Maximum_of_Luminosity  19219 non-null  int64  \n",
      " 11  Length_of_Conveyer     19219 non-null  int64  \n",
      " 12  TypeOfSteel_A300       19219 non-null  int64  \n",
      " 13  TypeOfSteel_A400       19219 non-null  int64  \n",
      " 14  Steel_Plate_Thickness  19219 non-null  int64  \n",
      " 15  Edges_Index            19219 non-null  float64\n",
      " 16  Empty_Index            19219 non-null  float64\n",
      " 17  Square_Index           19219 non-null  float64\n",
      " 18  Outside_X_Index        19219 non-null  float64\n",
      " 19  Edges_X_Index          19219 non-null  float64\n",
      " 20  Edges_Y_Index          19219 non-null  float64\n",
      " 21  Outside_Global_Index   19219 non-null  float64\n",
      " 22  LogOfAreas             19219 non-null  float64\n",
      " 23  Log_X_Index            19219 non-null  float64\n",
      " 24  Log_Y_Index            19219 non-null  float64\n",
      " 25  Orientation_Index      19219 non-null  float64\n",
      " 26  Luminosity_Index       19219 non-null  float64\n",
      " 27  SigmoidOfAreas         19219 non-null  float64\n",
      " 28  Pastry                 19219 non-null  int64  \n",
      " 29  Z_Scratch              19219 non-null  int64  \n",
      " 30  K_Scatch               19219 non-null  int64  \n",
      " 31  Stains                 19219 non-null  int64  \n",
      " 32  Dirtiness              19219 non-null  int64  \n",
      " 33  Bumps                  19219 non-null  int64  \n",
      " 34  Other_Faults           19219 non-null  int64  \n",
      "dtypes: float64(13), int64(22)\n",
      "memory usage: 5.1 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/steelplate.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "b3a4d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72039806",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\n",
    "    \"Pastry\",\n",
    "    \"Z_Scratch\",\n",
    "    \"K_Scatch\",\n",
    "    \"Stains\",\n",
    "    \"Dirtiness\",\n",
    "    \"Bumps\",\n",
    "    \"Other_Faults\",\n",
    "]\n",
    "\n",
    "Y = df.iloc[:, -7:].values.argmax(axis=1)\n",
    "\n",
    "Y = pd.DataFrame(Y, columns=[\"target_class\"])\n",
    "X = df.drop(columns=[\"id\"] + target_cols)\n",
    "\n",
    "# X = df.iloc[:, :-7].values\n",
    "# Y = df.iloc[:, -7:].values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "122f5d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_Minimum</th>\n",
       "      <th>X_Maximum</th>\n",
       "      <th>Y_Minimum</th>\n",
       "      <th>Y_Maximum</th>\n",
       "      <th>Pixels_Areas</th>\n",
       "      <th>X_Perimeter</th>\n",
       "      <th>Y_Perimeter</th>\n",
       "      <th>Sum_of_Luminosity</th>\n",
       "      <th>Minimum_of_Luminosity</th>\n",
       "      <th>Maximum_of_Luminosity</th>\n",
       "      <th>...</th>\n",
       "      <th>Outside_X_Index</th>\n",
       "      <th>Edges_X_Index</th>\n",
       "      <th>Edges_Y_Index</th>\n",
       "      <th>Outside_Global_Index</th>\n",
       "      <th>LogOfAreas</th>\n",
       "      <th>Log_X_Index</th>\n",
       "      <th>Log_Y_Index</th>\n",
       "      <th>Orientation_Index</th>\n",
       "      <th>Luminosity_Index</th>\n",
       "      <th>SigmoidOfAreas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>584</td>\n",
       "      <td>590</td>\n",
       "      <td>909972</td>\n",
       "      <td>909977</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2274</td>\n",
       "      <td>113</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2041</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.0104</td>\n",
       "      <td>0.1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>808</td>\n",
       "      <td>816</td>\n",
       "      <td>728350</td>\n",
       "      <td>728372</td>\n",
       "      <td>433</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>44478</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6365</td>\n",
       "      <td>0.7782</td>\n",
       "      <td>1.7324</td>\n",
       "      <td>0.7419</td>\n",
       "      <td>-0.2997</td>\n",
       "      <td>0.9491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>192</td>\n",
       "      <td>2212076</td>\n",
       "      <td>2212144</td>\n",
       "      <td>11388</td>\n",
       "      <td>705</td>\n",
       "      <td>420</td>\n",
       "      <td>1311391</td>\n",
       "      <td>29</td>\n",
       "      <td>141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0564</td>\n",
       "      <td>2.1790</td>\n",
       "      <td>2.2095</td>\n",
       "      <td>-0.0105</td>\n",
       "      <td>-0.0944</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>781</td>\n",
       "      <td>789</td>\n",
       "      <td>3353146</td>\n",
       "      <td>3353173</td>\n",
       "      <td>210</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>3202</td>\n",
       "      <td>114</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.3222</td>\n",
       "      <td>0.7782</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.4025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1540</td>\n",
       "      <td>1560</td>\n",
       "      <td>618457</td>\n",
       "      <td>618502</td>\n",
       "      <td>521</td>\n",
       "      <td>72</td>\n",
       "      <td>67</td>\n",
       "      <td>48231</td>\n",
       "      <td>82</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7694</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>1.8808</td>\n",
       "      <td>0.9158</td>\n",
       "      <td>-0.2455</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X_Minimum  X_Maximum  Y_Minimum  Y_Maximum  Pixels_Areas  X_Perimeter  \\\n",
       "0        584        590     909972     909977            16            8   \n",
       "1        808        816     728350     728372           433           20   \n",
       "2         39        192    2212076    2212144         11388          705   \n",
       "3        781        789    3353146    3353173           210           16   \n",
       "4       1540       1560     618457     618502           521           72   \n",
       "\n",
       "   Y_Perimeter  Sum_of_Luminosity  Minimum_of_Luminosity  \\\n",
       "0            5               2274                    113   \n",
       "1           54              44478                     70   \n",
       "2          420            1311391                     29   \n",
       "3           29               3202                    114   \n",
       "4           67              48231                     82   \n",
       "\n",
       "   Maximum_of_Luminosity  ...  Outside_X_Index  Edges_X_Index  Edges_Y_Index  \\\n",
       "0                    140  ...           0.0059         1.0000         1.0000   \n",
       "1                    111  ...           0.0044         0.2500         1.0000   \n",
       "2                    141  ...           0.1077         0.2363         0.3857   \n",
       "3                    134  ...           0.0044         0.3750         0.9310   \n",
       "4                    111  ...           0.0192         0.2105         0.9861   \n",
       "\n",
       "   Outside_Global_Index  LogOfAreas  Log_X_Index  Log_Y_Index  \\\n",
       "0                   0.0      1.2041       0.9031       0.6990   \n",
       "1                   1.0      2.6365       0.7782       1.7324   \n",
       "2                   0.0      4.0564       2.1790       2.2095   \n",
       "3                   1.0      2.3222       0.7782       1.4314   \n",
       "4                   1.0      2.7694       1.4150       1.8808   \n",
       "\n",
       "   Orientation_Index  Luminosity_Index  SigmoidOfAreas  \n",
       "0            -0.5000           -0.0104          0.1417  \n",
       "1             0.7419           -0.2997          0.9491  \n",
       "2            -0.0105           -0.0944          1.0000  \n",
       "3             0.6667           -0.0402          0.4025  \n",
       "4             0.9158           -0.2455          0.9998  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "9a3338a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_class\n",
       "0             3\n",
       "1             6\n",
       "2             2\n",
       "3             2\n",
       "4             6"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "74701836",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = X.select_dtypes(\n",
    "    include=[\n",
    "        \"int64\",\n",
    "        \"float64\",\n",
    "    ]\n",
    ").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "54d1a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", ppc.StandardScaler()),\n",
    "        # (\n",
    "        #     \"transformer\",\n",
    "        #     ppc.SplineTransformer(\n",
    "        #         n_knots=7,\n",
    "        #         degree=7,\n",
    "        #         order=\"F\",\n",
    "        #         extrapolation=\"linear\",\n",
    "        #     ),\n",
    "        # ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # (\"poly\", ppc.PolynomialFeatures(2)),\n",
    "        (\n",
    "            \"reg\",\n",
    "            RandomForestRegressor(\n",
    "                n_jobs=5,\n",
    "                n_estimators=100,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "43475ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.3,\n",
    "    random_state=69,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "410e6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poslam/Downloads/projects/fefu/7/ds/.venv/lib/python3.13/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "516e6523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.285, 2.35 , 2.06 , ..., 4.49 , 4.105, 2.38 ], shape=(3000,))"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "00122986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 2, ..., 6, 6, 2], shape=(3000,))"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[\"target_class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "2778819e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[722]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/projects/fefu/7/ds/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/projects/fefu/7/ds/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:359\u001b[39m, in \u001b[36maccuracy_score\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    358\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/projects/fefu/7/ds/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:106\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    103\u001b[39m     y_type = {\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mClassification metrics can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m targets\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    108\u001b[39m             type_true, type_pred\n\u001b[32m    109\u001b[39m         )\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[32m    113\u001b[39m y_type = y_type.pop()\n",
      "\u001b[31mValueError\u001b[39m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test[\"target_class\"].values, y_pred))\n",
    "# print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710380e4",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "8619baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./data/steelplate.csv\")\n",
    "\n",
    "df = df.dropna(axis=1)\n",
    "X = df.iloc[:, :-7].values\n",
    "y = df.iloc[:, -7:].values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train loss: 1.2105\n",
      "Validation loss: 1.0533\n",
      "Epoch 002 | Train loss: 1.1030\n",
      "Validation loss: 1.0408\n",
      "Epoch 003 | Train loss: 1.0818\n",
      "Validation loss: 1.0277\n",
      "Epoch 004 | Train loss: 1.0724\n",
      "Validation loss: 1.0284\n",
      "Epoch 005 | Train loss: 1.0585\n",
      "Validation loss: 1.0213\n",
      "Epoch 006 | Train loss: 1.0595\n",
      "Validation loss: 1.0213\n",
      "Epoch 007 | Train loss: 1.0519\n",
      "Validation loss: 1.0227\n",
      "Epoch 008 | Train loss: 1.0486\n",
      "Validation loss: 1.0158\n",
      "Epoch 009 | Train loss: 1.0452\n",
      "Validation loss: 1.0175\n",
      "Epoch 010 | Train loss: 1.0445\n",
      "Validation loss: 1.0147\n",
      "Epoch 011 | Train loss: 1.0420\n",
      "Validation loss: 1.0140\n",
      "Epoch 012 | Train loss: 1.0398\n",
      "Validation loss: 1.0138\n",
      "Epoch 013 | Train loss: 1.0372\n",
      "Validation loss: 1.0103\n",
      "Epoch 014 | Train loss: 1.0385\n",
      "Validation loss: 1.0133\n",
      "Epoch 015 | Train loss: 1.0307\n",
      "Validation loss: 1.0118\n",
      "Epoch 016 | Train loss: 1.0382\n",
      "Validation loss: 1.0115\n",
      "Epoch 017 | Train loss: 1.0316\n",
      "Validation loss: 1.0160\n",
      "Epoch 018 | Train loss: 1.0267\n",
      "Validation loss: 1.0055\n",
      "Epoch 019 | Train loss: 1.0229\n",
      "Validation loss: 1.0139\n",
      "Epoch 020 | Train loss: 1.0260\n",
      "Validation loss: 1.0079\n",
      "Epoch 021 | Train loss: 1.0250\n",
      "Validation loss: 1.0086\n",
      "Epoch 022 | Train loss: 1.0189\n",
      "Validation loss: 1.0040\n",
      "Epoch 023 | Train loss: 1.0171\n",
      "Validation loss: 1.0072\n",
      "Epoch 024 | Train loss: 1.0137\n",
      "Validation loss: 1.0050\n",
      "Epoch 025 | Train loss: 1.0184\n",
      "Validation loss: 1.0030\n",
      "Epoch 026 | Train loss: 1.0106\n",
      "Validation loss: 1.0012\n",
      "Epoch 027 | Train loss: 1.0080\n",
      "Validation loss: 0.9994\n",
      "Epoch 028 | Train loss: 1.0063\n",
      "Validation loss: 1.0032\n",
      "Epoch 029 | Train loss: 1.0064\n",
      "Validation loss: 1.0040\n",
      "Epoch 030 | Train loss: 1.0109\n",
      "Validation loss: 1.0052\n",
      "Epoch 031 | Train loss: 1.0053\n",
      "Validation loss: 1.0016\n",
      "Epoch 032 | Train loss: 1.0057\n",
      "Validation loss: 1.0042\n",
      "Epoch 033 | Train loss: 1.0022\n",
      "Validation loss: 1.0037\n",
      "Epoch 034 | Train loss: 1.0005\n",
      "Validation loss: 0.9983\n",
      "Epoch 035 | Train loss: 1.0036\n",
      "Validation loss: 0.9992\n",
      "Epoch 036 | Train loss: 0.9988\n",
      "Validation loss: 1.0005\n",
      "Epoch 037 | Train loss: 0.9977\n",
      "Validation loss: 0.9960\n",
      "Epoch 038 | Train loss: 1.0034\n",
      "Validation loss: 1.0010\n",
      "Epoch 039 | Train loss: 0.9958\n",
      "Validation loss: 0.9963\n",
      "Epoch 040 | Train loss: 0.9960\n",
      "Validation loss: 1.0020\n",
      "Epoch 041 | Train loss: 0.9928\n",
      "Validation loss: 1.0050\n",
      "Epoch 042 | Train loss: 0.9923\n",
      "Validation loss: 0.9994\n",
      "Epoch 043 | Train loss: 0.9871\n",
      "Validation loss: 1.0011\n",
      "Epoch 044 | Train loss: 0.9891\n",
      "Validation loss: 0.9973\n",
      "Epoch 045 | Train loss: 0.9913\n",
      "Validation loss: 0.9928\n",
      "Epoch 046 | Train loss: 0.9888\n",
      "Validation loss: 0.9969\n",
      "Epoch 047 | Train loss: 0.9897\n",
      "Validation loss: 1.0072\n",
      "Epoch 048 | Train loss: 0.9854\n",
      "Validation loss: 1.0004\n",
      "Epoch 049 | Train loss: 0.9834\n",
      "Validation loss: 1.0049\n",
      "Epoch 050 | Train loss: 0.9821\n",
      "Validation loss: 1.0022\n",
      "Epoch 051 | Train loss: 0.9791\n",
      "Validation loss: 1.0044\n",
      "Epoch 052 | Train loss: 0.9796\n",
      "Validation loss: 0.9955\n",
      "Epoch 053 | Train loss: 0.9761\n",
      "Validation loss: 0.9983\n",
      "Epoch 054 | Train loss: 0.9782\n",
      "Validation loss: 0.9976\n",
      "Epoch 055 | Train loss: 0.9812\n",
      "Validation loss: 1.0009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.17      0.25       457\n",
      "           1       0.58      0.64      0.61       230\n",
      "           2       0.86      0.91      0.89       686\n",
      "           3       0.70      0.88      0.78       114\n",
      "           4       0.39      0.20      0.26        97\n",
      "           5       0.53      0.52      0.52       952\n",
      "           6       0.47      0.57      0.52      1308\n",
      "\n",
      "    accuracy                           0.57      3844\n",
      "   macro avg       0.57      0.55      0.55      3844\n",
      "weighted avg       0.57      0.57      0.56      3844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = SteelDataset(X_train, y_train)\n",
    "test_dataset = SteelDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class DeepSteelModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = DeepSteelModel(input_dim=X.shape[1], output_dim=7)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "best_loss = np.inf\n",
    "patience = 10\n",
    "counter = 0\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1:03d} | Train loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            preds = model(Xb)\n",
    "            val_loss += criterion(preds, yb).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        preds = model(Xb)\n",
    "        y_true.extend(yb.numpy())\n",
    "        y_pred.extend(preds.argmax(dim=1).numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
